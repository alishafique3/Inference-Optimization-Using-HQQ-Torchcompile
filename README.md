# Accelerating LLaMA Inference: 4-bit Quantization + TorchCompile Using PrunaAI
[In progress]

This project has applied inference optimizations on Metaâ€™s LLaMA-3.2-1B-Instruct using Pruna AI â€” an optimization framework to make model faster, smaller, cheaper, greener.

## ğ—©ğ—®ğ—»ğ—¶ğ—¹ğ—¹ğ—® ğ— ğ—¼ğ—±ğ—²ğ—¹ ğ—¦ğ—²ğ˜ğ˜‚ğ—½
- Compute dtype: bfloat16
- Full Model Precision - FP32

## ğ—£ğ—¿ğ˜‚ğ—»ğ—®ğ—”ğ—œ ğ—¢ğ—½ğ˜ğ—¶ğ—ºğ—¶ğ˜‡ğ—®ğ˜ğ—¶ğ—¼ğ—» ğ—¦ğ—²ğ˜ğ˜‚ğ—½
- Compute dtype: bfloat16
- 4-bit HQQ quantization (Hessian-aware)
- torch.compile with max-autotune + full graph mode

## ğŸ“Š ğ—¥ğ—²ğ˜€ğ˜‚ğ—¹ğ˜ğ˜€
- Vanilla Inference Time: 0.754 sec
- Optimized Inference Time: 0.228 sec

â†’ Thatâ€™s a 3.3Ã— speedup on real hardware ğŸš€

![HF_vs_PrunaAI](https://github.com/user-attachments/assets/7ac3487d-1eb6-4de8-abb8-6f9be0856db0)


## Setup:
NVIDIA A40 (Runpod)
runpod/pytorch:2.4.0-py3.11-cuda12.4.1-devel-ubuntu22.04

## References
- [Pruna AI Tutorials](https://github.com/PrunaAI/pruna/tree/main/docs/tutorials)
